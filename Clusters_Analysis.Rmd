---
title: "Cluster Analysis"
author: "Shreyash Mehta"
date: "03/05/2023"
output:
html_document: default
word_document: default
pdf_document: default
---
  
```{r}

library(cluster)
library(readr)
library(factoextra)
library(magrittr)
library(NbClust)

iris <- read_csv("D:/Rutgers MITA/Spring 2023/MVA/Assignment 3/Iris.csv")
iris
str(iris)
attach(iris)
summary(iris)

#Hierarchical Clustering Analysis

#Distance measure
iris_dist <- get_dist(iris[,-5], stand = TRUE, method = "euclidean")
iris_dist

iris_nn <- hclust(iris_dist, method = "single")
plot(iris_nn, hang=-1,xlab="Object",ylab="Distance",
     main="Dendrogram. Nearest neighbor linkage")

iris_fn <- hclust(iris_dist)
plot(iris_nn, hang=-1,xlab="Object",ylab="Distance",
     main="Dendrogram. Farthest neighbor linkage")


iris_fn <- hclust(iris_dist)
plot(iris_nn, hang=-1,xlab="Object",ylab="Distance",
     main="Dendrogram. Group average linkage")


plot(as.dendrogram(iris_nn),ylab="Distance between each species",ylim=c(0,2.5),main="Dendrogram of 3 Species")

plot(as.dendrogram(iris_nn),ylab="Distance between each species",xlim =c(6,0),main="Dendrogram of 3 Species")


#-----------------------------------------
#K-Means Clustering


matstd_iris <- scale(iris[,-5])
matstd_iris

(kmeans2.employ <- kmeans(matstd_iris,3,nstart = 10))
(kmeans2.employ <- kmeans(matstd_iris,4,nstart = 10))
(kmeans2.employ <- kmeans(matstd_iris,5,nstart = 10))
(kmeans2.employ <- kmeans(matstd_iris,6,nstart = 10))


iris.dist <- get_dist(iris[,-5], stand = TRUE, method = "pearson")

head(iris.dist,n=3)#Pearson method
head(iris_dist,n=3)#Euclidean method


fviz_dist(iris.dist, 
          gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))


fviz_nbclust(matstd_iris, kmeans, method = "gap_stat")

set.seed(123)
km.res <- kmeans(matstd_iris, 3, nstart = 25)
# Visualize
fviz_cluster(km.res, data = matstd_iris,
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal())


pam.res <- pam(matstd_iris, 3)
# Visualize
fviz_cluster(pam.res)  


set.seed(123)

# Determining the optimal numbers of Clusters
library("NbClust")
res.nbclust <- iris[,-5] %>%
  scale() %>%
  NbClust(distance = "euclidean",
          min.nc = 2, max.nc = 5, 
          method = "complete", index ="all") 


fviz_nbclust <- function (x, FUNcluster = NULL, method = c("silhouette", "wss", 
                                                           "gap_stat"), diss = NULL, k.max = 10, nboot = 100, verbose = interactive(), 
                          barfill = "steelblue", barcolor = "steelblue", linecolor = "steelblue", 
                          print.summary = TRUE, ...) 
{
  set.seed(123)
  if (k.max < 2) 
    stop("k.max must bet > = 2")
  method = match.arg(method)
  if (!inherits(x, c("data.frame", "matrix")) & !("Best.nc" %in% 
                                                  names(x))) 
    stop("x should be an object of class matrix/data.frame or ", 
         "an object created by the function NbClust() [NbClust package].")
  if (inherits(x, "list") & "Best.nc" %in% names(x)) {
    best_nc <- x$Best.nc
    if (any(class(best_nc) == "numeric") ) 
      print(best_nc)
    else if (any(class(best_nc) == "matrix") )
      .viz_NbClust(x, print.summary, barfill, barcolor)
  }
  else if (is.null(FUNcluster)) 
    stop("The argument FUNcluster is required. ", "Possible values are kmeans, pam, hcut, clara, ...")
  else if (!is.function(FUNcluster)) {
    stop("The argument FUNcluster should be a function. ", 
         "Check if you're not overriding the specified function name somewhere.")
  }
  else if (method %in% c("silhouette", "wss")) {
    if (is.data.frame(x)) 
      x <- as.matrix(x)
    if (is.null(diss)) 
      diss <- stats::dist(x)
    v <- rep(0, k.max)
    if (method == "silhouette") {
      for (i in 2:k.max) {
        clust <- FUNcluster(x, i, ...)
        v[i] <- .get_ave_sil_width(diss, clust$cluster)
      }
    }
    else if (method == "wss") {
      for (i in 1:k.max) {
        clust <- FUNcluster(x, i, ...)
        v[i] <- .get_withinSS(diss, clust$cluster)
      }
    }
    df <- data.frame(clusters = as.factor(1:k.max), y = v, 
                     stringsAsFactors = TRUE)
    ylab <- "Total Within Sum of Square"
    if (method == "silhouette") 
      ylab <- "Average silhouette width"
    p <- ggpubr::ggline(df, x = "clusters", y = "y", group = 1, 
                        color = linecolor, ylab = ylab, xlab = "Number of clusters k", 
                        main = "Optimal number of clusters")
    if (method == "silhouette") 
      p <- p + geom_vline(xintercept = which.max(v), linetype = 2, 
                          color = linecolor)
    return(p)
  }
  else if (method == "gap_stat") {
    extra_args <- list(...)
    gap_stat <- cluster::clusGap(x, FUNcluster, K.max = k.max, 
                                 B = nboot, verbose = verbose, ...)
    if (!is.null(extra_args$maxSE)) 
      maxSE <- extra_args$maxSE
    else maxSE <- list(method = "firstSEmax", SE.factor = 1)
    p <- fviz_gap_stat(gap_stat, linecolor = linecolor, 
                       maxSE = maxSE)
    return(p)
  }
}

.viz_NbClust <- function (x, print.summary = TRUE, barfill = "steelblue", 
                          barcolor = "steelblue") 
{
  best_nc <- x$Best.nc
  if (any(class(best_nc) == "numeric") )
    print(best_nc)
  else if (any(class(best_nc) == "matrix") ) {
    best_nc <- as.data.frame(t(best_nc), stringsAsFactors = TRUE)
    best_nc$Number_clusters <- as.factor(best_nc$Number_clusters)
    if (print.summary) {
      ss <- summary(best_nc$Number_clusters)
      cat("Among all indices: \n===================\n")
      for (i in 1:length(ss)) {
        cat("*", ss[i], "proposed ", names(ss)[i], 
            "as the best number of clusters\n")
      }
      cat("\nConclusion\n=========================\n")
      cat("* According to the majority rule, the best number of clusters is ", 
          names(which.max(ss)), ".\n\n")
    }
    df <- data.frame(Number_clusters = names(ss), freq = ss, 
                     stringsAsFactors = TRUE)
    p <- ggpubr::ggbarplot(df, x = "Number_clusters", 
                           y = "freq", fill = barfill, color = barcolor) + 
      labs(x = "Number of clusters k", y = "Frequency among all indices", 
           title = paste0("Optimal number of clusters - k = ", 
                          names(which.max(ss))))
    return(p)
  }
}
# assign them to the factoextra namespace
environment(fviz_nbclust) <- asNamespace("factoextra")
assignInNamespace("fviz_nbclust",fviz_nbclust,"factoextra")
environment(.viz_NbClust) <- asNamespace("factoextra")
assignInNamespace(".viz_NbClust",.viz_NbClust,"factoextra")
fviz_nbclust(res.nbclust, ggtheme = theme_minimal())

library(factoextra)
fviz_nbclust(res.nbclust, ggtheme = theme_minimal())

class(res.nbclust)


# Quality of Clustering

set.seed(123)
# Enhanced hierarchical clustering, cut in 3 groups
res.hc <- iris[, -5] %>% scale() %>%
  eclust("hclust", k = 3, graph = FALSE)

# Visualize with factoextra
fviz_dend(res.hc, palette = "jco",
          rect = TRUE, show_labels = FALSE)

fviz_silhouette(res.hc)

# Silhouette width of observations
sil <- res.hc$silinfo$widths[, 1:3]

# Objects with negative silhouette
neg_sil_index <- which(sil[, 'sil_width'] < 0)
sil[neg_sil_index, , drop = FALSE]


employ <- read.csv("D:/Rutgers MITA/Spring 2023/MVA/Euroemp.csv",header=TRUE,row.names=1)
attach(employ)
(agn.employ <- agnes(employ, metric="euclidean", stand=TRUE, method = "single"))
plot(agn.employ, which.plots=2)
plot(agn.employ, which.plots=1)
#When which.plots=1, the plot function generates a dendrogram with the observations (or data points) labeled at the bottom of the plot. This plot shows the hierarchical structure of the clustering, with the height of the branches indicating the distance between the clusters.
#When which.plots=2, the plot function generates a plot of the agglomerative coefficients, which are the distances between clusters at each step of the clustering process. This plot shows how the distances between clusters change as more observations are merged into larger clusters.

matstd.employ <- scale(employ[,2:10])

(kmeans2.employ <- kmeans(matstd.employ,2,nstart = 10))
perc.var.2 <- round(100*(1 - kmeans2.employ$betweenss/kmeans2.employ$totss),1)
names(perc.var.2) <- "Perc. 2 clus"
perc.var.2
(kmeans3.employ <- kmeans(matstd.employ,3,nstart = 10))
perc.var.3 <- round(100*(1 - kmeans3.employ$betweenss/kmeans3.employ$totss),1)
names(perc.var.3) <- "Perc. 3 clus"
perc.var.3
(kmeans4.employ <- kmeans(matstd.employ,4,nstart = 10))
perc.var.4 <- round(100*(1 - kmeans4.employ$betweenss/kmeans4.employ$totss),1)
names(perc.var.4) <- "Perc. 4 clus"
perc.var.4
Variance_List <- c(perc.var.2,perc.var.3,perc.var.4)
Variance_List
plot(Variance_List)

fviz_nbclust(matstd.employ, kmeans, method = "gap_stat")
